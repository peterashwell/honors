\section{Introduction}
A \emph{shapelet} of a particular time series object within a dataset that is distinctive to the class of which the time series belongs. Distinctive to a class means that it allows us to tell time series of that class apart from time series that are not, by some method. Shapelets were first introduced in~\citet{ye2009time} and built upon in~\citet{mueen2011logical}. The first paper proposed information theoretic and distance measure based definitions of \emph{distinctness} in a 2-class context, giving an algorithm for Shapelet extraction. The second paper generalised to a multi-class classification context and proposed performance improvements. Both papers suggest a decision tree based classifier using \emph{subsequence distance} (figure~\ref{fig:subsequencedistance}) to training cases as features to build training rules. \\

Shapelets are interesting to the transient classification problem because they should be effective for classification and they are locale-independent. All previous methods explored rely on knowing the start and end points of an event to get good classification accuracy. Some transients (IDVs, XRBs, Flare stars) do not have well defined boundaries, so that is impossible. Additionally, sometimes only a fragment of a signal is available, and even if that fragment is highly characteristic of the class, no previous approach would function well unless it was trained explicitly on that fragment. Shapelets are robust in both of these scenarios and hence are worth exploring.

This chapter examines shapelets in the transient classification problem context, explores the basic algorithm in terms of classification performance, and finally proposes and compares modifications to both shapelet extraction and classification that may improve performance.

\section{Experiments}
There are some details related to the shapelet algorithm that need to be discussed before making proposals for how to incorporate them into a classifciation approach.
\begin{itemize}
	\item The Ye 2009  paper determines only one shapelet per class as that having the best information theoretic measure of discrimination. The second paper proposes a find the best single combination of shapelets per class. Neither paper outlines a way to find the best $N$ shapelets, very important for dealing with the high variability of our simulated data and the distortions we apply to it. A potential algorithm involving using clustering of shapelets and a user-defined clustering threshold of \emph{shapelet distinctness} would enable multiple shapelet discovery, with each cluster would be a separate shapelet.
	\item The distance measures for determining the similarity between time series and shapelet candidate subsequences is a slightly modified Euclidean distance, called the \emph{subsequence distance}. When there is even slight variability (but still a lot of similarity) amongst the phenomena we want the shapelet to represent, the distance measure will give poor results. A good example of such a phenomena is the sharp rise and peak of a Supernova transient. This structure occurs over many time scales and its peakiness means that unless a flexible distance measure is used, the shapelet will not be as useful as it could be. % TODO figure
	\item When extracting single shapelets, the multi-class entropy defined in \citep{mueen2011logical} may choose a shapelet giving a good split for a class besides the source class for a shapelet, if such a subsequence happens to exist. This means an entire class will have no highly representative shapelets. A remedy is to use a one-vs-all binary entropy for each class, changing the non-source class labels to, say, \emph{B}, and having the source class label as \emph{A}.  This forces the algorithm to choose a shapelet that works only for the source class, but may potentially miss useful shapelets for separating the dataset in more general ways. This is however essential in a single shapelet context. If multiple shapelets are used then multi-class entropy is preferable. % TODO figure for split lines
	\item Shapelets extracted from clean, normalised training data will not function well on distorted data. On clean data, even very subtle structures can be highly discriminative so long as they appear, in their subtlety, frequently within a class and not in others. These subtle shapelets become completely useless when noise is introduced (figure \ref{fig:noisyshapelet}. Similarly shapelets chosen from the latter part of light curves are useless if that part of the signal is not observed, and small shapelets with very strong variability are vulnerable to gappy data \ref{fig:gappyshapelet}. Clearly these complications will seriously hinder classifiction performance. A potential solution is to draw shapelets from clean light curves and find their discriminative power \emph{amongst distorted lightcurve datasets}. The algorithm would then ignore shapelets that are sensitive to poor data conditions, and in the case of limited data, would choose shapelets appearing early in the time series.
	\item Speed concerns, use limited dataset % TODO
\end{itemize}

The following sections outline the experiments leading out of the above discussion.
\setlength{\tabcolsep}{1.4pt}
\setlength{\extrarowheight}{1.8pt}

\subsection{Single shapelet per class}
%\subsection{cDTW}
%\subsection{Binary and multi-class entropy}
%\subsection{Distorted training sets}
\subsection{Multiple shapelets}
\clearpage
\section{Results}
\subsection{Introducing gaps into the light curve}
%\input{/Users/peter/honors/thesis/experiments/exp_shapelets/exp_improved/missing/results.tex}
\clearpage
\subsection{Limiting the amount of the light curve observed}
%\input{/Users/peter/honors/thesis/experiments/exp_shapelets/exp_improved/observed/results.tex}
\clearpage
%\subsection{Introducing noise into the light curve}
\input{/Users/peter/honors/thesis/experiments/exp_shapelets/exp_improved/noise/results.tex}
\clearpage
\subsection{Light curves with 1 in 2 datapoints missing, power law applied, clear signal within noise}
%\input{/Users/peter/honors/thesis/experiments/exp_shapelets/exp_improved/clear_alldist/results.tex}
\clearpage
\subsection{Light curves with 1 in 2 datapoints, power law applied, noisy signal}
%\input{/Users/peter/honors/thesis/experiments/exp_shapelets/exp_improved/noisy_alldist/results.tex}
\clearpage

\section{Discussion}
\subsection{Undistorted data}
The performance of both sets of shapelets on the undistorted set of lightcurves is shown in figures \ref{fig:gappymainplot}, \ref{fig:obsmainplot} and \ref{fig:noisymainplot} with an F-Score of approximately 0.58. This is a lower than expected result because there is so little deviation within the time series classes of the undisorted light curves. There are two possible explanations for this outcome:
\begin{enumerate}
	\item There are some classes that have no subsequences (subject to our length constraints [15, 40, 65, 90, 105]) that are separated by the distance measure from all the other classes.
	\item The shapelet extraction algorithm, trained on a subset of the training data, fails to choose general enough shapelets to accomodate slight variations in the testing data - a kind of overfitting.
\end{enumerate}
If the shapelet discrimination is poor on the training set, then the first issue is to blame for the poor performance. If the discrimination is good on the training set and poor on the test set, the second issue is to blame. I ran an additional experiment to evaluate the performance of the extracted shapelets on the exact training sets they were extracted from, with an F-Score of 


\begin{minipage}[c]{\textwidth}
		\vspace{4pt}
	\begin{minipage}[l]{\textwidth}
		\begin{minipage}[c]{0.09\textwidth}
			\centering
			10
		\end{minipage}
		\begin{minipage}[c]{0.03\textwidth}
			\tiny {
				\begin{tabular}{c>{\centering\arraybackslash} m{0pt}}
				ESE &\\
				ESE &\\
				ESE &\\
				ESE &\\
				ESE &\\
				ESE &\\
				ESE &\\
				ESE &\\
				\end{tabular}
			}
		\end{minipage}	
		\begin{minipage}[l]{0.27\textwidth}
			\centering
			\tiny {
			\begin{tabular}{|c|c|c|c|c|c|c|c|>{\centering\arraybackslash} m{0pt}}\hline
			1.0 & .95 & &x &x & &{1.00} & {1.00} &\\ \cline{1-8}
			{1.00}&{1.00}&{1.00}& {\texttt{1.00}}& {1.00}&{1.00}& &{1.00}&  \\ \cline{1-8}
			x &x &- &x &x & &{1.00} & {1.00} & \\  \cline{1-8}
			x & & &x &x &x &{1.00} \cellcolor[gray]{0.5}& {1.00} &\\ \cline{1-8}
			x &x & &x &x &x &{1.00}& {1.00} &\\  \hline
			x &x &- &x &x &x &{1.00}& {1.00} &\\  \cline{1-8}
			x & & &x &x &x &{1.00} & {1.00}  &\\ \cline{1-8}
			x &x & &x &x &x &{1.00}& {1.00} &\\ \cline{1-8}
			\end{tabular}
			}
		\end{minipage}
		\begin{minipage}[l]{0.27\textwidth}
			\centering
			\tiny {
			\begin{tabular}{|c|c|c|c|c|c|c|c|}\cline{1-8}
			1.0 	& .95 & 	&x 			&x & 		&{1.00} & {1.00} \\ \cline{1-8}
			{1.00}&	{1.00}	&{1.00}	& {\texttt{1.00}}& {1.00}&{1.00}& {1.00}& {1.00}  \\ \cline{1-8}
			x 		&	x 	&- 	&x 			&x & 		&{1.00} & {1.00} \\  \cline{1-8}
			x 		& & 	&x 	&x 			&x &{1.00} \cellcolor[gray]{0.5}& {1.00}\\ \cline{1-8}
			x 		&	x 	& 	&x 			&x &x &{1.00}& {1.00}\\  \hline
			x 		&	x 	&- 	&x 			&x &x &{1.00}& {1.00}\\  \cline{1-8}
			x 		&	 		& 	&x 			&x &x &{1.00} & {1.00} \\ \cline{1-8}
			x			&	x 	& 	&x 			&x &x &{1.00}& {1.00} \\ \cline{1-8}
			\end{tabular}
			}
		\end{minipage}
		\begin{minipage}[l]{0.27\textwidth}
			\centering
			\tiny {
			\begin{tabular}{|c|c|c|c|c|c|c|c|}\cline{1-8}
			1.0 	& .95 & 	&x 			&x & 		&{1.00} & {1.00} \\ \cline{1-8}
			{1.00}&	{1.00}	&{1.00}	& {\texttt{1.00}}& {1.00}&{1.00}& {1.00}& {1.00}  \\ \cline{1-8}
			x 		&	x 	&- 	&x 			&x & 		&{1.00} & {1.00} \\  \cline{1-8}
			x 		& & 	&x 	&x 			&x &{1.00} \cellcolor[gray]{0.5}& {1.00}\\ \cline{1-8}
			x 		&	x 	& 	&x 			&x &x &{1.00}& {1.00}\\  \hline
			x 		&	x 	&- 	&x 			&x &x &{1.00}& {1.00}\\  \cline{1-8}
			x 		&	 		& 	&x 			&x &x &{1.00} & {1.00} \\ \cline{1-8}
			x			&	x 	& 	&x 			&x &x &{1.00}& {1.00} \\ \cline{1-8}
			\end{tabular}
			}
		\end{minipage}
	\end{minipage}
\end{minipage}

\setlength{\tabcolsep}{5pt}
\setlength{\extrarowheight}{1pt}
\clearpage
\begin{minipage}[c]{\textwidth}
	
	\begin{minipage}[c]{\textwidth}
		\begin{minipage}[c]{0.09\textwidth}
			\centering
			0.0
		\end{minipage}
		\begin{minipage}[r]{0.06\textwidth}
			\tiny {
			\begin{tabular}	{c>{\centering\arraybackslash} m{0pt}}
				ESE & \\ 
				BG & \\ 
				XRB & \\ 
				FSdMe & \\ 
				FSRSCVn & \\ 
				Novae & \\ 
				IDV & \\ 
				SNe & \\ 
			\end{tabular}
			}
		\end{minipage}
		\begin{minipage}[c]{0.27\textwidth}
			\centering
			\tiny {
			\begin{tabular}{|c|c|c|c|c|c|c|c|}\hline
\cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 \\ \hline
\cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 \\ \hline
\cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 \\ \hline
\cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 \\ \hline
\cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 \\ \hline
\cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 \\ \hline
\cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 \\ \hline
\cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 & \cellcolor[rgb]{0,0,0} 1 \\ \hline
%			1.0 	& .95 & 	&x 			&x & 		&{1.00} & {1.00} \\ \hline
%			{1.00}&	{1.00}	&{1.00}	& {\texttt{1.00}}& {1.00}&{1.00}& {1.00}& {1.00}  \\ \hline
%			x 		&	x 	&- 	&x 			&x & 		&{1.00} & {1.00} \\  \hline
%			x 		& & 	&x 	&x 			&x &{1.00} \cellcolor[rgb]{1,0,0}& {1.00}\\ \hline
%			x 		&	x 	& 	&x 			&x &x &{1.00}& {1.00}\\  \hline
%			x 		&	x 	&- 	&x 			&x &x &{1.00}& {1.00}\\  \hline
%			x 		&	 		& 	&x 			&x &x &{1.00} & {1.00} \\ \hline
%			x			&	x 	& 	&x 			&x &x &{1.00}& {1.00} \\ \hline
			\end{tabular}
			}
		\end{minipage}
	\end{minipage}
\end{minipage}


\subsection{Missing data}
%\subsection{cDTW}
%\subsection{Binary and multi-class entropy}
%\subsection{Distorted training sets}
\subsection{Multiple shapelets}

\section{Conclusion}

