@book{rasmussen2006gpfml,
    author = {Rasmussen, Carl E. and Williams, Christopher},
    booktitle = {Gaussian Processes for Machine Learning},
    citeulike-article-id = {4015194},
    citeulike-linkout-0 = {http://www.gaussianprocess.org/gpml/},
    keywords = {gaussianprocesses, gp, machinelearning},
    posted-at = {2009-02-06 10:28:12},
    priority = {2},
    publisher = {MIT Press},
    title = {{Gaussian Processes for Machine Learning}},
    year = {2006}
}

@Article {rasmussen2010ssgpr,
   title = {Sparse Spectrum Gaussian Process Regression},
   journal = {Journal of Machine Learning Research},
   year = {2010},
   month = {6},
   volume = {11},
   pages = {1865-1881},
   abstract = {We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.},
   department = {Department Sch{\"o}lkopf},
   institute = {Biologische Kybernetik},
   organization = {Max-Planck-Gesellschaft},
   language = {en},
   author = {L{\'a}zaro-Gredilla, M and Qui{\~n}onero-Candela, J and Rasmussen, CE and Figueiras-Vidal, AR}
}

@ARTICLE{tresp2000bcm,
    author = {Volker Tresp},
    title = {A Bayesian Committee Machine},
    journal = {Neural Computation},
    year = {2000},
    volume = {12},
    pages = {2000}
}

@article{snelson2005sgppi,
    abstract = {{We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M N , where N is the number of real data points, and hence obtain a sparse regression method which has N) training cost and ) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian...}},
    author = {Snelson, E. and Ghahramani, Z.},
    booktitle = {NIPS 18},
    citeulike-article-id = {1846806},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.2209},
    journal = {Neural Information Processing Systems 18},
    keywords = {approximate, gaussianprocess, gp, machinelearning, sparse},
    posted-at = {2008-01-28 17:58:24},
    priority = {3},
    title = {{Sparse Gaussian Processes using Pseudo-inputs}},
    year = {2005}
}

@conference{walder2008sparse,
  title={{Sparse multiscale Gaussian process regression}},
  author={Walder, C. and Kim, K.I. and Sch{\\"o}lkopf, B.},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1112--1119},
  year={2008},
  organization={ACM}
}

@article{o1978curve,
  title={{Curve fitting and optimal design for prediction}},
  author={O'Hagan, A. and Kingman, JFC},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  volume={40},
  number={1},
  pages={1--42},
  issn={0035-9246},
  year={1978},
  publisher={JSTOR}
}
