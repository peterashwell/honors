\documentclass[10pt]{article}

\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{array}
\usepackage{subfigure}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage[all]{hypcap}
\usepackage{lscape}
\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{rotating}
\setlength{\parindent}{0in}

\usepackage[round]{natbib}
%\usepackage[colorlinks=true,citecolor=blue]{hyperref}
\usepackage{hypernat}

\bibliographystyle{plainnat}

\title{Exploring simple similarity measures for Astronomical time series}
\author{Peter Ashwell}
\date{}

\begin{document}
	\maketitle
	
	\section{Introduction}
		\label{sec:introduction}
		Similarity measures (chapter~\ref{sec:similarity}) combined with the nearest neighbour algorithm yield a simple approach to classification of astronomical transients. They are a preliminary investigation into the effectiveness of similarity measures for classification, and should also demonstrate some of the fundamental problems in this task. The two similarity measures to be compared are Euclidean, possibly the simplest similarity measure, and Dynamic Time Warping \citep{berndt1994using}, a dynamic programming approach. The results of this experiment will give a baseline with which to compare the performance of more sophisticated classification algorithms. These experiments follow the proecedure outlined in chapter~\ref{chap:experiment} for all the classification tasks. 
	
	\section{Procedure}
		\label{sec:procedure}
		The nearest neighbour algorithm uses a set of labeled data objects with a distance function, classifying unlabeled objects by the assumption that its true class is the same as its `closest' neighbours. The class in the majority taken from the $N$ nearest neighbours in the data set to the unlabeled object becomes its new label. In this experiment, $N$, the number of nearest neighbours used in the majority vote, will be 5. The algorithm is illustrated visually in figure~\ref{fig:nearestneighbor}.
		
		\begin{figure}[ht!]
			\label{fig:nearestneighbor}
			\centering
			Placeholder for nearest neighbour algorithm figure
			\caption{Nearest neighbour algorithm in binary classification. The black label is chosen for the new datapoint}
		\end{figure}
		
		The experiment will be carried out under a variety of data conditions (see chapter~\ref{chap:experiment}) with 10-fold cross validation to reduce overfitting. Euclidean distance cannot be applied when the time series has randomly missing datapoints, so these results are omitted. Dynamic Time Warping can be applied in all data conditions. Results will take the form of the precision, recall and F-score microaveraged across the individual results for the 6 classes. 

	\section{Euclidean distance}
		The simplest possible way to evaluate the similarity of two time series $x$ and $y$ is to compare the values $x_{i}$ and $y_{i}$ at each equal time index $i$ and compute the sum of the squared differences:
		
		\begin{equation}
			\sum\limits_{i = 0}^{l}(x_{i} - y_{i})^{2}
		\end{equation}
	
	Where $l$ is the length of $x$ and $y$. This is the Euclidean distance of the two time series, and is highly sensitive to every distortion present in astronomical time series: noise, time warping, amplitude scaling, outlier points. Additionally, this distance is not defined when some data points are missing or when the lengths of the two time series are not equal! In this experiment tests for missing data are not carried out, and a slightly more sophsticated Euclidean distance is used:
	
	\begin{equation}
		\arg\max\limits_{\theta \in \{0, \ldots, l\}}\sum\limits_{i = \theta \bmod l}^{s}(x_{i} - y_{i})^{2}
	\end{equation}
	
	Where $l$ and $s$ are the lengths of the longer and shorter time series respectively. This distance measure computes the Euclidean distance at all phase shifts of the two time series, and is defined when they are of unequal length. The computational complexity of each similarity calculation is $O(ls^{2})$. When used in the nearest neighbour algorithm gives overall complexity of $O(n^{2}ls^{2})$, approximately $O(n^{2}m^{3})$, where n is the number of time series in the dataset to be classified and $m$ is their average length.
	
	\section{Dynamic time warping}
	Dynamic time warping is a dynamic programming approach to time series classification that finds the optimal pairwise matching of the points in either series. All points in the time series are matched and no lines representing a match may cross. A dynamic time warping matching is represented visually in the figure~\ref{fig:dynamictimewarping}. 
	
	\begin{figure}[ht!]
		\label{fig:dynamictimewarping}
		\centering
		Placeholder for figure of dynamic time warping
		\caption{Optimal matching of time series indices found by dynamic time warping}
	\end{figure} 
	
	Time series under dynamic time warping lose all meaning in their time indices but their ordering. Time indices may be treated as having occurred all at once or at any rate so long as they stay in order, and all are matched to the comparison series. This makes dynamic time warping effective in coping with the time warping and time scaling issue present in astronomical time series, but it is sensitive to all other distortions and introduces new problems. The steepness of slopes, defined by a change in intensity over time, is no longer meaningful. Time series that are only defined by the steepness of rise of a particular component, such as Novae and SNe, will not appear dissimilar to DTW.
	
	\section{Results}
	Tables \ref{tab:individualresults} and \ref{tab:allresults} show the results for the individudal distortions and all distortions respectively.
	
	\begin{table}[ht!]
	\centering
	\begin{tabular}{|l|l|lll|}
%		\hline Description & Experiment conditions & Precision & Recall & F-Score \\ \hline 
%		\multirow{4}{*}{Varying distribution} 
%		& norm-n0-a100-m0-s400-dtw-0 & 0.74 & 0.742 & 0.74\\ 
%		& powlaw-n0-a100-m0-s400-dtw-0 & 0.755 & 0.755 & \textbf{0.753}\\ 
%		& norm-n0-a100-m0-s400-euclidean-0 & 0.708 & 0.36 & 0.421\\ 
%		& powlaw-n0-a100-m0-s400-euclidean-0 & 0.655 & 0.325 & 0.382\\ \hline
%		
%		\multirow{8}{*}{Varying available data}
%		& norm-n0-a5-m0-s400-dtw-0 & 0.413 & 0.373 & 0.383\\ 
%		& norm-n0-a10-m0-s400-dtw-0 & 0.518 & 0.465 & 0.478\\ 
%		& norm-n0-a30-m0-s400-dtw-0 & 0.678 & 0.64 & 0.648\\ 
%		& norm-n0-a50-m0-s400-dtw-0 & 0.714 & 0.713 & \textbf{0.713}\\ 
%		& norm-n0-a5-m0-s400-euclidean-0 & 0.22 & 0.108 & 0.132\\ 
%		& norm-n0-a10-m0-s400-euclidean-0 & 0.451 & 0.215 & 0.266\\ 
%		& norm-n0-a30-m0-s400-euclidean-0 & 0.569 & 0.342 & 0.409\\ 
%		& norm-n0-a50-m0-s400-euclidean-0 & 0.781 & 0.412 & 0.473\\ \hline
%		
%		\multirow{2}{*}{Adding noise}
%		& norm-n3-a100-m0-s400-dtw-1 & 0.762 & 0.755 & \textbf{0.757}\\ 
%		& norm-n3-a100-m0-s400-euclidean-0 & 0.691 & 0.338 & 0.401\\ \hline
%		
%		\multirow{6}{*}{All distortions}
%		& powlaw-n3-a10-m0-s400-dtw-0 & 0.546 & 0.517 & 0.523\\ 
%		& powlaw-n3-a30-m0-s400-dtw-0 & 0.687 & 0.668 & 0.675\\ 
%		& powlaw-n3-a50-m0-s400-dtw-0 & 0.72 & 0.705 & \textbf{0.708}\\ 
%		& powlaw-n3-a10-m0-s400-euclidean-0 & 0.604 & 0.303 & 0.359\\ 
%		& powlaw-n3-a30-m0-s400-euclidean-0 & 0.601 & 0.322 & 0.384\\ 
%		& powlaw-n3-a50-m0-s400-euclidean-0 & 0.764 & 0.333 & 0.408\\ \hline

	\hline Description & Experiment conditions & Precision & Recall & F-Score \\ \hline 


\multirow{2}{*}{No distortions}
 & norm-n0-a100-m0-s400-dtw-0 & 0.74 & 0.742 & \textbf{0.74} \\ 
 & norm-n0-a100-m0-s400-euclidean-0 & 0.708 & 0.36 & 0.421 \\ \hline

\multirow{2}{*}{Varying distribution}
 & powlaw-n0-a100-m0-s400-dtw-0 & 0.755 & 0.755 & \textbf{0.753} \\
 & powlaw-n0-a100-m0-s400-euclidean-0 & 0.655 & 0.325 & 0.382 \\ \hline

\multirow{3}{*}{Gapifying data} 
 & norm-n0-a100-m90-s400-dtw-0 & 0.642 & 0.628 & 0.627 \\
 & norm-n0-a100-m50-s400-dtw-0 & 0.721 & 0.717 & 0.715 \\ 
 & norm-n0-a100-m10-s400-dtw-0 & 0.726 & 0.722 & \textbf{0.722} \\ \hline

\multirow{8}{*}{Limiting observed part of light curve}
 & norm-n0-a5-m0-s400-dtw-0 & 0.413 & 0.373 & 0.383 \\ 
 & norm-n0-a10-m0-s400-dtw-0 & 0.518 & 0.465 & 0.478 \\ 
 & norm-n0-a30-m0-s400-dtw-0 & 0.678 & 0.64 & 0.648 \\ 
 & norm-n0-a50-m0-s400-dtw-0 & 0.714 & 0.713 & \textbf{0.713} \\ 
 & norm-n0-a5-m0-s400-euclidean-0 & 0.22 & 0.108 & 0.132 \\
 & norm-n0-a10-m0-s400-euclidean-0 & 0.451 & 0.215 & 0.266 \\ 
 & norm-n0-a30-m0-s400-euclidean-0 & 0.569 & 0.342 & 0.409 \\ 
 & norm-n0-a50-m0-s400-euclidean-0 & 0.781 & 0.412 & 0.473 \\ \hline


\multirow{4}{*}{Introducing noise}
 & norm-n3-a100-m0-s400-dtw-1 & 0.762 & 0.755 & \textbf{0.757} \\ 
 & norm-n6-a100-m0-s400-dtw-0 & 0.722 & 0.727 & 0.724 \\ 
 & norm-n3-a100-m0-s400-euclidean-0 & 0.691 & 0.338 & 0.401 \\ 
 & norm-n6-a100-m0-s400-euclidean-1 & 0.446 & 0.282 & 0.312 \\ \hline

	\end{tabular}
	\label{tab:individualresults}
	\caption{Classification results for individual distortion types. The best F-Scores in each category are marked in bold.}
	\end{table}

	\begin{table}[ht!]
	\centering
	\begin{tabular}{|l|l|lll|}
\hline Description & Experiment conditions & Precision & Recall & F-Score \\ \hline 
\multirow{6}{*}{All conditions (no missing data)}  
 & powlaw-n6-a10-m0-s400-dtw-0 & 0.518 & 0.493 & 0.499 \\ 
 & powlaw-n6-a30-m0-s400-dtw-0 & 0.672 & 0.642 & 0.652 \\ 
 & powlaw-n6-a50-m0-s400-dtw-0 & 0.746 & 0.743 & \textbf{0.743} \\ 
 & powlaw-n6-a10-m0-s400-euclidean-0 & 0.333 & 0.222 & 0.248 \\ 
 & powlaw-n6-a30-m0-s400-euclidean-0 & 0.39 & 0.28 & 0.324 \\ 
 & powlaw-n6-a50-m0-s400-euclidean-1 & 0.486 & 0.292 & 0.329 \\ \hline 

\multirow{3}{*}{All conditions (missing data)}
 & powlaw-n6-a10-m10-s400-dtw-0 & 0.528 & 0.492 & 0.5 \\ 
 & powlaw-n6-a30-m10-s400-dtw-0 & 0.672 & 0.643 & 0.652 \\ 
 & powlaw-n6-a50-m10-s400-dtw-0 & 0.705 & 0.708 & \textbf{0.706} \\ \hline
	\end{tabular}
	\label{tab:allresults}
	\caption{Classification results for all distortion types. The best F-Scores in each category are marked in bold.}
	\end{table}
		
	\section{Discussion}
	\subsection{Distortions and classification performance}
	Predictably, the performance of the classifier decreases with an increase in severity of every distortion type. However, with the introduction of a power law distribution and noise, the DTW performance increases slightly. The same is true of the Euclidean distance measure when half the data is removed. 
	\subsection{Characteristic errors}
	\subsection{Suggestions for improvement and future work}
		\begin{itemize}
			\item Convolution, why not?
			\item Does the total length of the light curve affect classification?
			\item Novae rise rate vs. Supernovae
			\item Padding with noise either side of an event	 
			\item Sampling footprints
		\end{itemize}
	 It is the same idea as convolution (TODO add reference), except that in that distance measure the dot product of the two values is used. The perf
	
	\section{Conclusion}
	
	\bibliography{../../refs}
\end{document}
